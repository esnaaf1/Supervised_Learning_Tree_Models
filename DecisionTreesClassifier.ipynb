{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Algorithm - Decision Trees, Tree Ensemble\n",
    "In this note book using the Heart Failure Prediction Dataset, we will experiment with Supervised Learning model Decision Trees and Tree Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/heart.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.Use one-hot encoding to convert the categorical variables such as Sex, RestingEEG to numeric\n",
    "\n",
    "one-hot encoding transforms a categorical variable with n output into n binary (0,1) variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_variables = ['Sex', 'ChestPainType', 'RestingECG', 'ExerciseAngina', 'ST_Slope']\n",
    "\n",
    "# use pd.get_dummies for one-hot encoding\n",
    "\n",
    "df = pd.get_dummies(data=df, prefix=cat_variables, columns = cat_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extract features from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the features form the dataset by removing the target variable HeartDisease\n",
    "features = [x for x in df.columns if x not in 'HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encoding increased the number of columns to 20 in the feature\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create input X\n",
    "X= df[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create target y\n",
    "y=df['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, train_size =.8, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'training samples: {len(X_train)}')\n",
    "print(f'validation samples: {len(X_val)}')\n",
    "print(f'target prpportion: {sum(y_train)/len(y_train):,.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.Build the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Decision trees using scikit-learn\n",
    "\n",
    "We will use the following hyperparamters:\n",
    "- min_sample_split:  The minimum number of samples required to split a internal note.  \n",
    "    - Choosing a higher value can reduce the number of splits; therefore help reduce overfitting.\n",
    "\n",
    "- max_depth:  The maximum depth of the tree\n",
    "    - Choosing a lower value can reduce the overall number of splits; therefore help reduce overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a set of values hyperparameters for min_sample_split and max_depth\n",
    "min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700]\n",
    "max_depth_list = [1, 2, 3, 4, 8, 16, 32, None]  # None means that there is no depth limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model with different min_samples_split values and measure the accuracy score\n",
    "\n",
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "\n",
    "for min_samples_split in min_samples_split_list:\n",
    "    \n",
    "    model = DecisionTreeClassifier(min_samples_split = min_samples_split,\n",
    "                                  random_state = 55).fit(X_train, y_train)\n",
    "    predictions_train = model.predict(X_train)  ## The predicted values for the train dataset\n",
    "    predictions_val = model.predict(X_val)   ## The predicated values for the validation dataset\n",
    "    accuracy_train = accuracy_score(predictions_train, y_train)\n",
    "    accuracy_val = accuracy_score(predictions_val, y_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs min_samples_split\n",
    "\n",
    "plt.title(\"Train x Validation metrics\")\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks(ticks = range(len(min_samples_split_list)), labels = min_samples_split_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that increasing min_samples_split will reduce the overfitting.  Train and validatio scores getting close to each other.  Notice that increasing it from 10 to 30 and 50 does not improve the validation accuracy my much it brings the training accuracy closer to it, showing a reduction in overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try the same experiment with the max_depth hyperparameter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build and fit the model with different max_depth values and measure the accuracy score\n",
    "\n",
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    \n",
    "    model = DecisionTreeClassifier(max_depth = max_depth,\n",
    "                                  random_state = 55).fit(X_train, y_train)\n",
    "    predictions_train = model.predict(X_train)  ## The predicted values for the train dataset\n",
    "    predictions_val = model.predict(X_val)   ## The predicated values for the validation dataset\n",
    "    accuracy_train = accuracy_score(predictions_train, y_train)\n",
    "    accuracy_val = accuracy_score(predictions_val, y_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs max_depth\n",
    "\n",
    "plt.title(\"Train x Validation metrics\")\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks(ticks = range(len(max_depth_list)), labels = max_depth_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend(['Train', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as max_depth decreases the validation accuracy improves and the it gets closer to the train accuracy\n",
    "A few observations:\n",
    "\n",
    "- Reducing max_depth from 8 to 4 increases validation accuracy closure the training accuracy, while significantly reducing training accuracy\n",
    "- when max-depth <3 both validation and training accuracies decrease\n",
    "- when max_depth >=5  validation accuracy decreases while training accuracy increases; thus indicating that the model is overfitting the trainig set\n",
    "\n",
    "\n",
    "Therefore, based on the above graphs we decide to choose th efollwing values for our model:\n",
    "- max_depth  = 4\n",
    "- min_samples_split = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the decision tree classifier model \n",
    "decision_tree_model = DecisionTreeClassifier(min_samples_split = 50, max_depth=4, random_state=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model to the training set\n",
    "decision_tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy\n",
    "print(\"Decision Tree Classifier metrics:\")\n",
    "print(f\"Medtrics train\\n\\tAccuracy score: {accuracy_score(decision_tree_model.predict(X_train),y_train):.4f}\")\n",
    "print(f\"Medtrics validation\\n\\tAccuracy score: {accuracy_score(decision_tree_model.predict(X_val),y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see there is no sign of overfitting, although the model is not very accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Random Forest\n",
    "Next we build a random forest model using scikit-learn implementation\n",
    "\n",
    "- In addition to all the hyperparameters found in the decision tree, Random Forest has a parameter called n_estimators.  n_estimatators determine the number of decision trees that make up the random forest.  \n",
    "\n",
    "- In Random Forest model we randomly select a subset of features and randomly select a subset of training examples to train each individual tree.  \n",
    "\n",
    "- ff n is the number of features, we select $\\sqrt(n)$ of these features to train each individual tree.\n",
    "- We will also use n_jobs parameter to fit more than one tree in parallel.  The higher the value of n_jobs will increase CPU cores in use\n",
    "\n",
    "- We will use n_estimators = [10, 50, 100] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples_split_list = [2,10, 30, 50, 100, 200, 300, 700]  ## If the number is an integer, then it is the actual quantity of samples,\n",
    "                                             ## If it is a float, then it is the percentage of the dataset\n",
    "max_depth_list = [2, 4, 8, 16, 32, 64, None]\n",
    "n_estimators_list = [10,50,100,500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we build and fit our model and capture accuracy scores for each value of the min_samples_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for min_samples_split in min_samples_split_list:\n",
    "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
    "    model = RandomForestClassifier(min_samples_split = min_samples_split,\n",
    "                                   random_state = 55).fit(X_train,y_train) \n",
    "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
    "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
    "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
    "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We plot accuracy vs min_samples_split\n",
    "plt.title('Train x Validation metrics')\n",
    "plt.xlabel('min_samples_split')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks(ticks = range(len(min_samples_split_list )),labels=min_samples_split_list) \n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the best value for the min_samples_split might be around 10. This is where the difference between training and validation has decreased showing less overfitting and the accuracy is close .90 for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try to build and fit the same model with different values of max_depth\n",
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for max_depth in max_depth_list:\n",
    "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
    "    model = RandomForestClassifier(max_depth = max_depth,\n",
    "                                   random_state = 55).fit(X_train,y_train) \n",
    "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
    "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
    "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
    "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs max_depth\n",
    "plt.title('Train x Validation metrics')\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks(ticks = range(len(max_depth_list )),labels=max_depth_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like anything less that max_depth 16 will reduce the accuracy of the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build and fit models for various values of the n_estimators and measure accuracy score\n",
    "accuracy_list_train = []\n",
    "accuracy_list_val = []\n",
    "for n_estimators in n_estimators_list:\n",
    "    # You can fit the model at the same time you define it, because the fit function returns the fitted estimator.\n",
    "    model = RandomForestClassifier(n_estimators = n_estimators,\n",
    "                                   random_state = 55).fit(X_train,y_train) \n",
    "    predictions_train = model.predict(X_train) ## The predicted values for the train dataset\n",
    "    predictions_val = model.predict(X_val) ## The predicted values for the test dataset\n",
    "    accuracy_train = accuracy_score(predictions_train,y_train)\n",
    "    accuracy_val = accuracy_score(predictions_val,y_val)\n",
    "    accuracy_list_train.append(accuracy_train)\n",
    "    accuracy_list_val.append(accuracy_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy vs. n_estimators\n",
    "plt.title('Train x Validation metrics')\n",
    "plt.xlabel('n_estimators')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks(ticks = range(len(n_estimators_list )),labels=n_estimators_list)\n",
    "plt.plot(accuracy_list_train)\n",
    "plt.plot(accuracy_list_val)\n",
    "plt.legend(['Train','Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the validation accuracy incresaes up to 100, then decreases after that.  So we will go with 100\n",
    "\n",
    "Therefore, we try to fit the random forest witht he following parameters:\n",
    "\n",
    "- max_depth = 16\n",
    "- min_samples_split: 10\n",
    "- n_estimators = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the Random Forest model\n",
    "random_forest_model = RandomForestClassifier(n_estimators = 100, max_depth=16, min_samples_split = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit the model to the training set\n",
    "random_forest_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We measure accuracy\n",
    "print(\"Random Forest Classifier Metrics:\")\n",
    "print(f\"Metrics train acurracy score: {accuracy_score(random_forest_model.predict(X_train), y_train)}\")\n",
    "print(f\"Metrics validation acurracy score: {accuracy_score(random_forest_model.predict(X_val), y_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idealy we want to check combination of values for every hyper parameter that we are tuning.  IF we have 3 hyper parameters and each has 5 values we have a total of 5X5X5 = 125 combinations.\n",
    "\n",
    "To try out all combinations we can use a sklearn implementation called GridSearchCV.  GridSearchCV has a refill parameter that will automatically refit a model on the best combination so we will not have to program it explicitly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_grid = {'max_depth': max_depth_list,\n",
    "               'min_samples_split': min_samples_split_list,\n",
    "               'n_estimators': n_estimators_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_model = RandomForestClassifier()\n",
    "grid_clf = GridSearchCV(random_forest_model, params_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GridSearch CV metrics:\")\n",
    "print(f\"Training accuracy: {accuracy_score(grid_clf.predict(X_train), y_train):.4f}\")\n",
    "print(f\"Validation accuracy: {accuracy_score(grid_clf.predict(X_val), y_val):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 XGBoost\n",
    "Next is the Gradient Boosting model, called XGBoost. The boosting methods train several trees, but instead of them being uncorrelated to each other, now the trees are fit one after the other in order to minimize the error.\n",
    "\n",
    "The model has the same parameters as a decision tree, plus the learning rate.\n",
    "\n",
    "The learning rate is the size of the step on the Gradient Descent method that the XGBoost uses internally to minimize the error on each train step.\n",
    "One interesting thing about the XGBoost is that during fitting, it can take in an evaluation dataset of the form (X_val,y_val).\n",
    "\n",
    "On each iteration, it measures the cost (or evaluation metric) on the evaluation datasets.\n",
    "Once the cost (or metric) stops decreasing for a number of rounds (called early_stopping_rounds), the training will stop.\n",
    "More iterations lead to more estimators, and more estimators can result in overfitting.\n",
    "By stopping once the validation metric no longer improves, we can limit the number of estimators created, and reduce overfitting.\n",
    "First, let's define a subset of our training set (we should not use the test set here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(len(X_train)*0.8) ## Let's use 80% to train and 20% to eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fit, X_train_eval, y_train_fit, y_train_eval = X_train[:n], X_train[n:], y_train[:n], y_train[n:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set a large number of estimators, because we can stop if the cost function stops decreasing.\n",
    "\n",
    "ote some of the `.fit()` parameters:\n",
    "- `eval_set = [(X_train_eval,y_train_eval)]`:Here we must pass a list to the eval_set, because you can have several different tuples ov eval sets. \n",
    "- `early_stopping_rounds`: This parameter helps to stop the model training if its evaluation metric is no longer improving on the validation set. It's set to 10.\n",
    "  - The model keeps track of the round with the best performance (lowest evaluation metric).  For example, let's say round 16 has the lowest evaluation metric so far.\n",
    "  - Each successive round's evaluation metric is compared to the best metric.  If the model goes 10 rounds where none have a better metric than the best one, then the model stops training.\n",
    "  - The model is returned at its last state when training terminated, not its state during the best round.  For example, if the model stops at round 26, but the best round was 16, the model's training state at round 26 is returned, not round 16.\n",
    "  - Note that this is different from returning the model's \"best\" state (from when the evaluation metric was the lowest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = XGBClassifier(n_estimators=500, learning_rate=0.1, verbosity=1, random_state=55, use_label_encoder=False)\n",
    "xgb_model.fit(X_train_fit, y_train_fit, eval_set = [(X_train_eval, y_train_eval)], early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we initialized the model to allow up to 500 estimators, the algorithm only fit 26 estimators (over 26 rounds of training).\n",
    "\n",
    "To see why, let's look for the round of training that had the best performance (lowest evaluation metric).  You can either view the validation log loss metrics that were output above, or view the model's `.best_iteration` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.best_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best round of training was round 16, with a log loss of 4.3948.  \n",
    "- For 10 rounds of training after that (from round 17 to 26), the log loss was higher than this.\n",
    "- Since we set `early_stopping_rounds` to 10, then by the 10th round where the log loss doesn't improve upon the best one, training stops.\n",
    "- You can try out different values of `early_stopping_rounds` to verify this.  If you set it to 20, for instance, the model stops training at round 36 (16 + 20)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XGBoost metrics:\")\n",
    "print(f\"Training acurracy: {accuracy_score(xgb_model.predict(X_train), y_train): .4f}\")\n",
    "print(f\"Validation acurracy: {accuracy_score(xgb_model.predict(X_val), y_val): .4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
